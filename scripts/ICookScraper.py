"""
iCook é£Ÿè­œçˆ¬èŸ²ï¼ˆé¡åˆ¥ç‰ˆæœ¬ï¼‰â”€â”€ å°‡ A çš„åŠŸèƒ½ã€è¼¸å‡ºæ ¼å¼æ•´åˆé€² B
author : ChatGPTâ€‘merged
date   : 2025â€‘07â€‘16
"""

import csv
import os
import random
import re
import time
from typing import Dict, List, Tuple
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# --------- å·¥å…·å‡½å¼ï¼ˆæª”æ¡ˆåˆæ³•åŒ–ã€ç¶²å€æ¸…æ´—ï¼‰ ----------------
_filename_re = re.compile(r'[\\/:*?"<>|]')  # Windows / Unix éƒ½ç¦ç”¨çš„å­—å…ƒ
_size_flag_re = re.compile(r"/[wh]:\d+/")  # iCook ç¸®åœ–ç¶²å€çš„å¯¬é«˜åƒæ•¸


def safe_name(text: str) -> str:
    """æŠŠæª”åè£¡ä¸å…è¨±çš„ç¬¦è™Ÿéƒ½æ›æˆåº•ç·š"""
    return _filename_re.sub("_", text).strip()


def full_image_url(url: str) -> str:
    """å»æ‰ä¾‹å¦‚ /w:200/ æˆ– /h:300/ çš„ç¸®åœ–é™åˆ¶ï¼Œæ‹¿åˆ°åŸåœ–"""
    return _size_flag_re.sub("/", url) if url else url


def load_keywords_from_txt(txt_path: str) -> list[str]:
    """
    è®€å–ç´”æ–‡å­—æª”ï¼Œæ¯è¡Œä¸€å€‹é—œéµå­—ï¼Œå»æ‰ç©ºè¡Œèˆ‡å‰å¾Œç©ºç™½
    """
    with open(txt_path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]


# --------- ä¸»é¡åˆ¥ ----------------------------------------------------------
class ICookRecipeScraper:
    def __init__(
        self,
        image_root: str = "images",
        headers: Dict[str, str] | None = None,
        base_url: str = "https://icook.tw",
    ):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update(
            headers
            or {
                "User-Agent": (
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0 Safari/537.36"
                )
            }
        )

        # åœ–ç‰‡ç¸½è³‡æ–™å¤¾ï¼ˆä¾é—œéµå­—å†åˆ†å­è³‡æ–™å¤¾ï¼‰
        self.image_root = image_root
        os.makedirs(self.image_root, exist_ok=True)

    # --------------------- ä¸€ã€æœå°‹åˆ—è¡¨ ----------------------------------
    def search_recipes(
        self, keyword: str, max_count: int = 3, page_limit: int = 1
    ) -> List[Dict]:
        """å›å‚³å« id/name/url/preview_ingredients/img_url çš„ dict æ¸…å–®"""

        results: List[Dict] = []

        for page in range(1, page_limit + 1):
            url = f"{self.base_url}/search/{keyword}?page={page}"
            res = self.session.get(url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")

            for card in soup.select("a.browse-recipe-link"):
                recipe: Dict = {}

                name_tag = card.select_one("h2.browse-recipe-name")
                preview_tag = card.select_one("p.browse-recipe-content-ingredient")
                article = card.select_one("article.browse-recipe-card")
                img_tag = card.select_one("img.browse-recipe-cover-img")
                href = card.get("href")

                # å®‰å…¨æª¢æŸ¥
                if not (name_tag and href and article):
                    continue

                recipe["id"] = article.get("data-recipe-id", "")
                recipe["name"] = name_tag.get_text(strip=True)
                recipe["url"] = urljoin(self.base_url, href)
                recipe["preview_ingredients"] = (
                    preview_tag.get_text(strip=True).replace("é£Ÿæï¼š", "")
                    if preview_tag
                    else ""
                )

                raw_img = (
                    img_tag.get("data-src") or img_tag.get("src") if img_tag else ""
                )
                recipe["img_url"] = full_image_url(raw_img)

                results.append(recipe)
                if len(results) >= max_count:
                    return results

            # é æ•¸è·‘å®Œæˆ–å·²é” max_count
            if len(results) >= max_count:
                break

            time.sleep(random.uniform(1, 2))  # ç¦®è²Œä¼‘æ¯

        return results

    # --------------------- äºŒã€æŠ“å–è©³ç´°é£Ÿè­œ --------------------------------
    def get_recipe_details(self, url: str) -> Tuple[List[str], List[str]]:
        """å›å‚³ ingredients list, steps list"""
        res = self.session.get(url, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        ingredients = []
        for li in soup.select("li.ingredient"):
            name = li.select_one(".ingredient-name")
            unit = li.select_one(".ingredient-unit")
            if name:
                ingredients.append(
                    f"{name.get_text(strip=True)}{unit.get_text(strip=True) if unit else ''}"
                )

        steps = [
            p.get_text(strip=True)
            for p in soup.select("p.recipe-step-description-content")
            if p.get_text(strip=True)
        ]

        return ingredients, steps

    # --------------------- ä¸‰ã€ä¸‹è¼‰åœ–ç‰‡ ------------------------------------
    def download_image(
        self, img_url: str, recipe_id: str, recipe_name: str, keyword: str
    ) -> str | None:
        """ä¸‹è¼‰åœ–ç‰‡åˆ° images/<keyword>/ï¼ŒæˆåŠŸå›å‚³è·¯å¾‘ï¼Œå¤±æ•—å› None"""

        if not img_url or img_url.startswith("data:image"):
            print(f"âš ï¸  è·³éç„¡æ•ˆåœ–ç‰‡ç¶²å€ï¼š{img_url}")
            return None

        try:
            res = self.session.get(img_url, timeout=10)
            res.raise_for_status()
            sub_dir = os.path.join(self.image_root, safe_name(keyword))
            os.makedirs(sub_dir, exist_ok=True)

            filename = f"{recipe_id}_{safe_name(recipe_name)}.jpg"
            path = os.path.join(sub_dir, filename)

            with open(path, "wb") as f:
                f.write(res.content)

            print(f"âœ…  åœ–ç‰‡å·²ä¸‹è¼‰ï¼š{path}")
            return path

        except Exception as e:
            print(f"âš ï¸  ä¸‹è¼‰åœ–ç‰‡å¤±æ•—ï¼š{e}")
            return None

    # --------------------- å››ã€å¯« CSV -------------------------------------
    @staticmethod
    def save_to_csv(
        data: List[Dict], keyword: str, csv_path: str | None = None
    ) -> None:
        """å­˜åŠå½¢åˆ†è™Ÿåˆ†éš”çš„ CSVï¼Œä¸¦å›ºå®šæ”¾åœ¨ recipe/ è³‡æ–™å¤¾"""

        # ---------- (1) æ±ºå®šè·¯å¾‘ ----------
        # å…ˆç¢ºä¿ recipe/ é€™å€‹è³‡æ–™å¤¾å­˜åœ¨
        recipe_dir = os.path.join("data", "raw")
        os.makedirs(recipe_dir, exist_ok=True)

        # CSV æª”åç¶­æŒåŸæœ¬è¦å‰‡
        filename = csv_path or f"{keyword}_é£Ÿè­œè³‡æ–™.csv"

        # æŠŠè³‡æ–™å¤¾ + æª”åçµ„æˆå®Œæ•´è·¯å¾‘
        full_path = os.path.join(recipe_dir, filename)

        # ---------- (2) å¯«æª” ----------
        with open(full_path, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(
                f,
                delimiter=";",  # æ¬„ä½ä»ç”¨åˆ†è™Ÿåˆ†éš”
                quotechar='"',
                quoting=csv.QUOTE_ALL,
            )
            writer.writerow(
                [
                    "id",
                    "é£Ÿè­œåç¨±",
                    "ç¶²å€",
                    "é è¦½é£Ÿæ",
                    "è©³ç´°é£Ÿæ",
                    "åšæ³•",
                    "åœ–ç‰‡ç›¸å°è·¯å¾‘",
                ]
            )
            for item in data:
                writer.writerow(
                    [
                        item.get("id", ""),
                        item.get("name", ""),
                        item.get("url", ""),
                        item.get("preview_ingredients", ""),
                        ", ".join(item.get("ingredients", [])),
                        " / ".join(item.get("steps", [])),
                        item.get("img_path", ""),
                    ]
                )

        print(f"\nâœ…  å·²å„²å­˜ {len(data)} ç­†è³‡æ–™ â†’ {full_path}")

    # --------------------- äº”ã€æ•´åˆæµç¨‹ï¼ˆå°å¤–å”¯ä¸€å…¥å£ï¼‰ --------------------
    def run(
        self,
        keywords: List[str] | str,
        max_recipes: int = 3,
        page_limit: int = 1,
    ) -> None:
        """keywords å¯çµ¦ 'é«˜éº—èœ' æˆ– ['é«˜éº—èœ', 'é¦¬éˆ´è–¯']"""

        if isinstance(keywords, str):
            keywords = [keywords]

        for kw in keywords:
            print(f"\nğŸ”  é–‹å§‹æœå°‹ï¼šã€Œ{kw}ã€")
            recipes = self.search_recipes(
                kw, max_count=max_recipes, page_limit=page_limit
            )
            print(f"â†’ å…±æ‰¾åˆ° {len(recipes)} ç­†\n")

            collected = []
            for idx, r in enumerate(recipes, 1):
                print(f"ğŸ½  ç¬¬ {idx} ç­†ï½œ{r['name']}")

                # 1) ä¸‹è¼‰åœ–ç‰‡
                img_path = self.download_image(r["img_url"], r["id"], r["name"], kw)
                r["img_path"] = img_path or ""

                # 2) è©³ç´°å…§å®¹
                try:
                    r["ingredients"], r["steps"] = self.get_recipe_details(r["url"])
                except Exception as e:
                    print(f"âš ï¸  æŠ“è©³ç´°å¤±æ•—ï¼š{e}")
                    r["ingredients"], r["steps"] = [], []

                collected.append(r)
                print("-" * 40)
                time.sleep(random.uniform(1, 2))

            # 3) è¼¸å‡º CSV
            self.save_to_csv(collected, kw)


# -------------------- è‹¥ç›´æ¥åŸ·è¡Œæ­¤æª” -------------------------
if __name__ == "__main__":
    # --- è®€ TXT å–å¾—æ‰€æœ‰è”¬èœåç¨± ---
    TXT_FILE = "data\è”¬èœ.txt"  # â† è‹¥æª”åæˆ–è·¯å¾‘ä¸åŒï¼Œé€™è£¡æ”¹
    keywords = load_keywords_from_txt(TXT_FILE)

    # --- å»ºç«‹çˆ¬èŸ²ç‰©ä»¶ ---
    scraper = ICookRecipeScraper(image_root="data/images")

    # --- é€ä¸€çˆ¬å– ---
    # å‚³ list é€²å»å³å¯ï¼›ç¨‹å¼æœƒå°æ¯å€‹é—œéµå­—å„è‡ªè¼¸å‡º CSV èˆ‡åœ–ç‰‡è³‡æ–™å¤¾
    scraper.run(
        keywords=keywords,
        max_recipes=50,  # æ¯ç¨®è”¬èœè¦æŠ“å¹¾ç¯‡é£Ÿè­œï¼ŒæŒ‰éœ€èª¿æ•´
        page_limit=3,  # æœå°‹çµæœç¿»å¹¾é 
    )
