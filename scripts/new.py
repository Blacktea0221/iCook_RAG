#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
new.py

æ ¹æ®æœ€æ–°æ–‡ä»¶ç»“æž„å’Œåˆ†ç±»é€»è¾‘æ”¹å†™åŽçš„ search_and_retrieve_recipes è„šæœ¬ï¼š
- embeddings å…ƒæ–‡ä»¶ç›´æŽ¥æ”¾åœ¨ data/embeddings/ ä¸‹ï¼Œä¸å†æŒ‰è”¬èœå­æ–‡ä»¶å¤¹ç»„ç»‡
- åŠ è½½ index.json ç”¨äºŽæœªæ¥åŠ é€Ÿï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰
- åŠ è½½ Meat and Vegetarian.json è¿›è¡Œ diet ä¸Ž uses_pork è¿‡æ»¤
- ä¸»æµç¨‹ï¼šé€šè¿‡å­ä¸²æ£€æµ‹åˆ†ç±»æ§½ä½ â†’ Jieba åˆ†è¯æŠ½å–é£Ÿææ§½ä½ â†’ åˆ†ç±»è¿‡æ»¤ â†’ å‘é‡æ£€ç´¢ â†’ ç»“æžœè¿‡æ»¤ â†’ æ™ºèƒ½æŽ¨è & è¯¦æƒ…æŸ¥è¯¢
"""
import json
import os
import re
import subprocess
import textwrap
from collections import defaultdict
from typing import Dict, List

import jieba
import numpy as np
import pandas as pd
from googlesearch import search  # pip install googlesearch-python
from sentence_transformers import SentenceTransformer

# -------------------- é¡¹ç›®è·¯å¾„ --------------------
vege_name = "ä¹å±¤å¡”"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)

# -------------------- æ–‡ä»¶è·¯å¾„ --------------------
# embeddings ç›¸å…³æ–‡ä»¶ç›´æŽ¥æ”¾åœ¨ data/embeddings ä¸‹
tags_path = os.path.join(ROOT_DIR, "data", "embeddings", "tags.json")
embed_path = os.path.join(ROOT_DIR, "data", "embeddings", "embeddings.npy")
index_path = os.path.join(ROOT_DIR, "data", "embeddings", "index.json")
classify_path = os.path.join(ROOT_DIR, "data", "embeddings", "Meat and Vegetarian.json")

# æ¸…æ´—æ•°æ®ä»æŒ‰ vege_name å­˜å‚¨
cleaned_path = os.path.join(
    ROOT_DIR, "data", "clean", vege_name, f"{vege_name}_recipes_cleaned.csv"
)
preview_path = os.path.join(
    ROOT_DIR, "data", "clean", vege_name, f"{vege_name}_preview_ingredients.csv"
)
detailed_path = os.path.join(
    ROOT_DIR, "data", "clean", vege_name, f"{vege_name}_detailed_ingredients.csv"
)
steps_path = os.path.join(
    ROOT_DIR, "data", "clean", vege_name, f"{vege_name}_recipe_steps.csv"
)

# -------------------- åŠ è½½ embeddings ä¸Žæ¨¡åž‹ --------------------
with open(tags_path, "r", encoding="utf-8") as f:
    tags = json.load(f)
embeddings = np.load(embed_path)
model = SentenceTransformer("BAAI/bge-m3")
emb_norms = np.linalg.norm(embeddings, axis=1)

# åŠ è½½ index.jsonï¼ˆæœªæ¥å¯ç”¨äºŽåŠ é€Ÿç‰¹å®šé£Ÿææ£€ç´¢ï¼‰
with open(index_path, "r", encoding="utf-8") as f:
    index_map = json.load(f)

# åŠ è½½ Meat and Vegetarian åˆ†ç±»æ–‡ä»¶
with open(classify_path, "r", encoding="utf-8") as f:
    classification = json.load(f)

# å»ºç«‹ id -> set(tag)
id2tags = defaultdict(set)
for item in tags:
    rid = int(item["id"])
    id2tags[rid].add(item["tag"])

# -------------------- åŠ è½½æ¸…æ´—åŽçš„é£Ÿè°±æ•°æ® --------------------
print("è½½å…¥æ¸…æ´—åŽçš„é£Ÿè°±æ•°æ®...")
df_cleaned = pd.read_csv(cleaned_path, sep=";", encoding="utf-8-sig")
df_cleaned.columns = df_cleaned.columns.str.strip()
df_preview = pd.read_csv(preview_path, encoding="utf-8-sig").rename(columns=lambda x: x.strip())
df_detailed = pd.read_csv(detailed_path, encoding="utf-8-sig").rename(columns=lambda x: x.strip())
df_steps = pd.read_csv(steps_path, encoding="utf-8-sig").rename(columns=lambda x: x.strip())

# ==============================================================
# æž„å»ºé£Ÿæå­—å…¸ & Jieba è¯å…¸
# ==============================================================
def build_ingredient_set(df_preview: pd.DataFrame, df_detailed: pd.DataFrame) -> set:
    tags_set = set()
    for line in df_preview["preview_tag"]:
        tags_set.update(t.strip() for t in str(line).split(",") if t.strip())
    tags_set.update(df_detailed["ingredient_name"].astype(str).str.strip())
    return {t for t in tags_set if t and not re.fullmatch(r"\d+", t)}

ING_SET = build_ingredient_set(df_preview, df_detailed)
for w in ING_SET:
    jieba.add_word(w)
print(f"é£Ÿæå­—å…¸å¤§å°ï¼š{len(ING_SET)}")

# ==============================================================
# åˆ†ç±»æ§½ä½ & æ˜ å°„
# ==============================================================
CLASS_DICT = {"ç´ é£Ÿ", "è‘·é£Ÿ"}
CLASS_MAPPING = {"ç´ é£Ÿ": "vegetarian", "è‘·é£Ÿ": "non_vegetarian"}

# ==============================================================
# å…³é”®å­—æŠ½å–å‡½æ•°
# ==============================================================
LLM_PROMPT = textwrap.dedent(
    """
    ä½ æ˜¯é£ŸææŠ½å–åŠ©æ‰‹ï¼Œåªå›ž JSON æ•°ç»„ã€‚è¯·ä»Žå¥å­ä¸­æå–é£Ÿæåç§°ï¼š
    ---
    {text}
    ---
    """
)

def jieba_extract(text: str) -> List[str]:
    clean = re.sub(r"[^\w\u4e00-\u9fff]+", " ", text)
    tokens = jieba.lcut(clean, cut_all=False)
    return [ing for ing in ING_SET if ing in text]

def llm_extract(text: str, model_name: str = "qwen3:4b-q4_K_M") -> List[str]:
    prompt = LLM_PROMPT.format(text=text)
    proc = subprocess.run([...],
                    capture_output=True,
                    text=True,
                    encoding="utf-8",
                    errors="ignore")
    proc = subprocess.run(["ollama","run",model_name,prompt],
                        capture_output=True,
                        text=True,
                        encoding="utf-8",
                        errors="ignore")
    raw = proc.stdout or ""
    try:
        items = json.loads(raw)
    except (json.JSONDecodeError, TypeError):
        items = re.split(r"[ï¼Œ,]\s*", raw)
    return [i.strip() for i in items if i.strip() in ING_SET]

def pull_ingredients(user_text: str) -> List[str]:
    words = jieba_extract(user_text)
    return words if words else llm_extract(user_text)

# ==============================================================
# å‘é‡æ£€ç´¢ & éƒ¨åˆ†åŒ¹é…
# ==============================================================
def search_by_partial_ingredients(query: str, top_k: int = 3):
    ingredients = [kw.strip() for kw in query.replace("ï¼Œ", ",").split(",") if kw.strip()]
    if not ingredients:
        return []
    id2count = {}
    for rid, tagset in id2tags.items():
        count = sum(any(kw in tag for tag in tagset) for kw in ingredients)
        if count>0:
            id2count[rid]=count
    if not id2count:
        return []
    q_emb = model.encode([query])[0]
    q_norm = np.linalg.norm(q_emb)
    sims = embeddings.dot(q_emb)/(emb_norms*q_norm+1e-10)
    id2score = {}
    for i,t in enumerate(tags):
        rid=int(t["id"])
        if rid in id2count:
            id2score[rid]=max(id2score.get(rid,float("-inf")),float(sims[i]))
    sorted_ids = sorted(id2count.keys(), key=lambda rid:(-id2count[rid],-id2score[rid]))[:top_k]
    results=[]
    for rid in sorted_ids:
        rec=get_recipe_by_id(rid)
        if rec:
            results.append({"id":rid,"score":id2score[rid],"recipe":rec})
    return results

# ==============================================================
# èŽ·å–å•ä¸ªé£Ÿè°±è¯¦æƒ…
# ==============================================================
def get_recipe_by_id(recipe_id: int)->Dict:
    rec=df_cleaned[df_cleaned["id"]==recipe_id]
    if rec.empty: return None
    rd=rec.iloc[0].to_dict()
    rd["preview_tags"]=df_preview[df_preview["id"]==recipe_id]["preview_tag"].tolist()
    det=df_detailed[df_detailed["id"]==recipe_id]
    rd["ingredients"]=det[["ingredient_name","quantity","unit"]].to_dict(orient="records")
    st=df_steps[df_steps["id"]==recipe_id].sort_values("step_no")
    rd["steps"]=st[["step_no","description"]].to_dict(orient="records")
    return rd

# ==============================================================
# è°ƒç”¨ Ollama æŽ¨è
# ==============================================================
def call_ollama_llm(user_query:str,recipes:List[Dict],model_name:str="qwen3:4b-q4_K_M") ->str:
    if not recipes: return "æ‰¾ä¸åˆ°ç¬¦åˆçš„é£Ÿè­œã€‚"
    blocks=[]
    for r in recipes:
        rec=r["recipe"]
        ingr="ã€".join(i["ingredient_name"] for i in rec["ingredients"])
        blocks.append(f"ã€{rec['é£Ÿè­œåç¨±']}ã€‘(ID:{r['id']}) ä¸»è¦é£Ÿæï¼š{ingr}")
    ctx="\n\n---\n\n".join(blocks)
    prompt=textwrap.dedent(f"""
        ä»¥ä¸‹æ˜¯æ–™ç†è³‡è¨Šï¼š
        {ctx}
        è«‹æ‰®æ¼”æ–™ç†å°ˆå®¶ï¼Œç”¨æ¢åˆ—å¼æŽ¨è–¦æœ€é©åˆ \"{user_query}\" çš„é£Ÿè­œï¼Œæ¯æ¢30å­—ä»¥å…§ï¼Œæ¨™è¨»åç¨±èˆ‡IDã€‚
        ç”¨ç¹é«”ä¸­æ–‡ã€‚
    """
    )
    res=subprocess.run(["ollama","run",model_name,prompt],capture_output=True,text=True)
    return res.stdout.strip() if res.returncode==0 else f"OllamaéŒ¯èª¤: {res.stderr.strip()}"

# ==============================================================
# Google åŽå¤‡
# ==============================================================
def google_search_recipes(keyword:str,k:int=5)->List[Dict]:
    query=f"{keyword} é£Ÿè­œ"
    return [{"title":item.title,"link":item.url,"snippet":item.description} for item in search(query,num_results=k,lang="zh-tw")]

def summarize_search_results(user_query:str,results:List[Dict],model_name:str="qwen3:4b-q4_K_M")->str:
    blocks=[f"ã€{r['title']}ã€‘\n{r['snippet']}\nLink:{r['link']}" for r in results]
    ctx="\n\n---\n\n".join(blocks)
    prompt=textwrap.dedent(f"""
        ä»¥ä¸‹æ˜¯Googleæœç´¢ã€Œ{user_query}é£Ÿè­œã€çµæžœï¼Œè«‹ç”¨æ¢åˆ—æ ¼å¼è¼¸å‡ºæ¨™é¡Œã€20å­—å…§ç°¡ä»‹ã€ç¶²å€ã€‚
        ç”¨ç¹é«”ä¸­æ–‡ã€‚
        {ctx}
    """
    )
    res=subprocess.run(["ollama","run",model_name,prompt],capture_output=True,text=True)
    return res.stdout.strip()

# ==============================================================
# å¯è¯»è¾“å‡º
# ==============================================================
def pretty_print(item:Dict):
    rec=item['recipe']
    print(f"===ID{item['id']} ç›¸ä¼¼åº¦{item['score']:.4f}===")
    print(f"åç¨±ï¼š{rec['é£Ÿè­œåç¨±']} åˆ†é¡žï¼š{rec.get('vege_name','')}")
    print("â”€â”€é£Ÿæâ”€â”€")
    for i,ing in enumerate(rec['ingredients'],1): print(f"{i}.{ing['ingredient_name']}{ing['quantity']}{ing['unit']}")
    print("â”€â”€æ­¥é©Ÿâ”€â”€")
    for s in rec['steps']: print(f"{s['step_no']}.{s['description']}")

# ==============================================================
# ä¸»æµç¨‹
# ==============================================================
if __name__=='__main__':
    print("RAG æ™ºèƒ½æŽ¨è–¦ (exité›¢é–‹)")
    while True:
        raw=input("\nè«‹æè¿°ä½ æœ‰çš„é£Ÿææˆ–éœ€æ±‚: ").strip()
        if raw.lower() in ("exit","quit"): break

        # åˆ†ç±»æ£€æµ‹
        classes=[cls for cls in CLASS_DICT if cls in raw]
        hates_pork="ä¸åƒè±¬è‚‰" in raw
        allowed_ids=None
        if classes:
            diet=CLASS_MAPPING[classes[0]]
            allowed_ids={int(rid) for rid,v in classification.items() if v.get('diet')==diet}
            if hates_pork: allowed_ids={rid for rid in allowed_ids if not classification.get(str(rid),{}).get('uses_pork')}
        elif hates_pork:
            allowed_ids={int(rid) for rid,v in classification.items() if not v.get('uses_pork')}

        # æŠ½å–å…³é”®è¯
        keywords=pull_ingredients(raw)
        if not keywords:
            if allowed_ids is not None:
                print(f"ðŸ” æ‰¾åˆ° {len(allowed_ids)} é“{classes[0] if classes else ''}é£Ÿè­œ, é¡¯ç¤ºå‰5ç­†ï¼š")
                valid=[]
                for rid in allowed_ids:
                    rec=get_recipe_by_id(rid)
                    if rec: valid.append((rid,rec))
                    if len(valid)>=5: break
                for rid,rec in valid:
                    print(f"- {rec.get('é£Ÿè­œåç¨±','')} (ID: {rid})")
                continue
            print("âš ï¸ æœªæª¢æ¸¬åˆ°é£Ÿæï¼ŒGoogleåŽå¤‡...")
            web=google_search_recipes(raw)
            if not web: print("ðŸš« æ— ç»“æžœ"); continue
            print(summarize_search_results(raw,web))
            continue

        # æœ¬åœ°æ£€ç´¢
        query=", ".join(keywords)
        candidates=search_by_partial_ingredients(query)
        if allowed_ids is not None:
            candidates=[c for c in candidates if c['id'] in allowed_ids]
        if not candidates:
            print("âš ï¸ æœ¬åœ°æ— ç»“æžœï¼ŒGoogleåŽå¤‡...")
            web=google_search_recipes(query)
            if not web: print("ðŸš« æ— ç»“æžœ"); continue
            print(summarize_search_results(query,web))
            continue

        # æŽ¨èä¸Žè¯¦æƒ…
        print("\næŽ¨èï¼š")
        print(call_ollama_llm(query,candidates))
        print("\nðŸ” è¾“å…¥ID/åç§°æŸ¥çœ‹æ›´å¤š; newé‡æ–°; exité€€å‡º")
        id_map={c['recipe']['é£Ÿè­œåç¨±']:c['id'] for c in candidates}
        while True:
            sel=input("> ").strip()
            if sel.lower() in('exit','quit'): exit()
            if sel.lower()=='new': break
            rid=None
            if sel.isdigit(): rid=int(sel)
            else:
                for name,idx in id_map.items():
                    if sel in name: rid=idx; break
            if rid and any(c['id']==rid for c in candidates):
                pretty_print(next(c for c in candidates if c['id']==rid))
            else: print("æ— æ³•è¯†åˆ«")
