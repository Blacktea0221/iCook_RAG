#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
search_and_retrieve_recipes.py

æ•´åˆæ–‡å­—æª¢ç´¢èˆ‡å®Œæ•´é£Ÿè­œå›å‚³åŠŸèƒ½ï¼Œä¸¦ä»¥æ˜“è®€æ ¼å¼è¼¸å‡ºï¼š
1. è¼‰å…¥ embeddings èˆ‡ metadata
2. è¼‰å…¥æ¸…ç†å¾Œçš„é£Ÿè­œè³‡æ–™
3. æä¾› search_similar(query, top_k) èˆ‡ search_and_retrieve å‡½å¼
4. CLI äº’å‹•å¼è¼¸å…¥ï¼Œä¸¦ä»¥è‡ªè¨‚æ ¼å¼åˆ—å°çµæœï¼ˆä¸å«é è¦½æ¨™ç±¤ï¼‰

ä½¿ç”¨å‰è«‹å®‰è£ä¾è³´ï¼š
$ pip install pandas numpy sentence-transformers googlesearch-python jieba

åŸ·è¡Œï¼š
$ python search_and_retrieve_recipes.py
"""
import json
import os
import re
import subprocess
import textwrap
from collections import defaultdict
from typing import Dict, List

import jieba
import numpy as np
import pandas as pd
from googlesearch import search  # pip install googlesearch-python
from sentence_transformers import SentenceTransformer

# -------------------- å°ˆæ¡ˆè·¯å¾‘ --------------------
category = "ä¹å±¤å¡”"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(BASE_DIR)

# -------------------- æª”æ¡ˆè·¯å¾‘ --------------------
tags_path = os.path.join(ROOT_DIR, "data", "embeddings", category, "tags.json")
embed_path = os.path.join(ROOT_DIR, "data", "embeddings", category, "embeddings.npy")
cleaned_path = os.path.join(
    ROOT_DIR, "data", "clean", category, f"{category}_recipes_cleaned.csv"
)
preview_path = os.path.join(
    ROOT_DIR, "data", "clean", category, f"{category}_preview_ingredients.csv"
)
detailed_path = os.path.join(
    ROOT_DIR, "data", "clean", category, f"{category}_detailed_ingredients.csv"
)
steps_path = os.path.join(
    ROOT_DIR, "data", "clean", category, f"{category}_recipe_steps.csv"
)

# -------------------- è¼‰å…¥å‘é‡èˆ‡æ¨¡å‹ --------------------
with open(tags_path, "r", encoding="utf-8") as f:
    tags = json.load(f)
embeddings = np.load(embed_path)
model = SentenceTransformer("BAAI/bge-m3")
emb_norms = np.linalg.norm(embeddings, axis=1)

# å»ºç«‹ id -> set(tag)
id2tags = defaultdict(set)
for item in tags:
    id2tags[item["id"]].add(item["tag"])

# -------------------- è¼‰å…¥é£Ÿè­œè³‡æ–™ --------------------
print("è¼‰å…¥æ¸…ç†å¾Œçš„é£Ÿè­œè³‡æ–™...")
df_cleaned = pd.read_csv(cleaned_path, sep=";", encoding="utf-8-sig")
df_cleaned.columns = df_cleaned.columns.str.strip()
df_preview = pd.read_csv(preview_path, encoding="utf-8-sig").rename(
    columns=lambda x: x.strip()
)
df_detailed = pd.read_csv(detailed_path, encoding="utf-8-sig").rename(
    columns=lambda x: x.strip()
)
df_steps = pd.read_csv(steps_path, encoding="utf-8-sig").rename(
    columns=lambda x: x.strip()
)

# ==============================================================
#  â˜…â˜…â˜… æ–°å¢å€å¡Š 1ï¼šæº–å‚™ã€Œé£Ÿæå­—å…¸ã€ â˜…â˜…â˜…
# ==============================================================


def build_ingredient_set(df_preview: pd.DataFrame, df_detailed: pd.DataFrame) -> set:
    """å¾ preview_tag èˆ‡ ingredient_name å…©æ¬„çµ„æˆå»é‡å¾Œçš„é£Ÿæé›†åˆ"""
    tags_set = set()
    # preview_tag ä»¥é€—è™Ÿåˆ†éš”
    for line in df_preview["preview_tag"]:
        tags_set.update(t.strip() for t in str(line).split(",") if t.strip())
    # detailed_ingredients
    tags_set.update(df_detailed["ingredient_name"].astype(str).str.strip())
    # å»ç©ºã€å»ç´”æ•¸å­—
    return {t for t in tags_set if t and not re.fullmatch(r"\d+", t)}


# â˜… åœ¨ build_ingredient_set å®šç¾©ä¹‹å¾Œã€main loop ä¹‹å‰åŠ ä¸Š â†“â†“â†“
ING_SET: set[str] = build_ingredient_set(df_preview, df_detailed)

# æŠŠæ‰€æœ‰é£ŸæåŠ é€² Jieba è‡ªè¨‚å­—å…¸ï¼Œè®“æ–·è©èƒ½ä¸€æ¬¡åˆ‡å‡ºå®Œæ•´è©
for w in ING_SET:
    jieba.add_word(w)

print(f"é£Ÿæå­—å…¸å¤§å°ï¼š{len(ING_SET)}")

# ==============================================================
#  â˜…â˜…â˜… æ–°å¢å€å¡Š 2ï¼šé—œéµå­—æŠ½å–å‡½å¼ â˜…â˜…â˜…
# ==============================================================

LLM_PROMPT = """ä½ æ˜¯é£ŸææŠ½å–åŠ©æ‰‹ï¼Œåªå› JSON é™£åˆ—ã€‚å¾å¥å­ä¸­æ‰¾å‡ºé£Ÿæåç¨±ï¼ˆåªè¦åç¨±ï¼‰ï¼Œä¾åºè¼¸å‡ºï¼š
---
{text}
---"""


def jieba_extract(text: str) -> List[str]:
    """ç”¨ Jieba æ–·è©å¾Œéç™½åå–®"""
    clean = re.sub(r"[^\w\u4e00-\u9fff]+", " ", text)
    tokens = jieba.lcut(clean, cut_all=False)
    return [ing for ing in ING_SET if ing in text]


def llm_extract(text: str, model_name: str = "qwen3:4b-q4_K_M") -> List[str]:
    """å‘¼å« Ollama æ¨¡å‹æŠ½å–é—œéµå­—ï¼ˆå¾Œæ´ï¼‰"""
    prompt = LLM_PROMPT.format(text=text)
    res = subprocess.run(
        ["ollama", "run", model_name, prompt],
        capture_output=True,
        text=True,
        encoding="utf-8",
    ).stdout
    try:
        items = json.loads(res)
    except json.JSONDecodeError:
        items = re.split(r"[ï¼Œ,]\s*", res)
    # åªç•™å­—å…¸å…§è©
    return [i.strip() for i in items if i.strip() in ING_SET]


def pull_ingredients(user_text: str) -> List[str]:
    """å…ˆç”¨ Jiebaï¼Œæ¯”å°ä¸åˆ°å†ç”¨ LLMï¼›å›å‚³é£Ÿææ¸…å–®"""
    words = jieba_extract(user_text)
    return words if words else llm_extract(user_text)


def get_recipe_by_id(recipe_id, dfs):
    """
    æ ¹æ“š recipe_id å¾å·²è¼‰å…¥çš„ DataFrame ä¸­å–å¾—å®Œæ•´é£Ÿè­œè³‡æ–™ã€‚
    è¿”å›ä¸€å€‹ dict åŒ…æ‹¬ï¼š
      - ä¸»è¡¨æ¬„ä½
      - preview_tagsï¼ˆlistï¼‰
      - ingredientsï¼ˆlist of dictï¼‰
      - stepsï¼ˆlist of dictï¼‰
    """
    df_recipes, df_preview, df_detailed, df_steps = dfs

    # å–å‡ºé£Ÿè­œä¸»è¡¨çš„ä¸€ç­†è³‡æ–™
    rec = df_recipes[df_recipes["id"] == recipe_id]
    if rec.empty:
        return None
    rec_dict = rec.iloc[0].to_dict()

    # é è¦½é£Ÿæåˆ—è¡¨
    tags = df_preview[df_preview["id"] == recipe_id]["preview_tag"].tolist()

    # è©³ç´°é£Ÿæåˆ—è¡¨
    det = df_detailed[df_detailed["id"] == recipe_id]
    ingredients = det[["ingredient_name", "quantity", "unit"]].to_dict(orient="records")

    # åšæ³•æ­¥é©Ÿ
    st = df_steps[df_steps["id"] == recipe_id].sort_values("step_no")
    steps_list = st[["step_no", "description"]].to_dict(orient="records")

    # çµ„åˆçµæœ
    rec_dict["preview_tags"] = tags
    rec_dict["ingredients"] = ingredients
    rec_dict["steps"] = steps_list

    return rec_dict


def search_similar(query: str, top_k: int = 5):
    """
    è¿”å›èˆ‡æŸ¥è©¢å­—ä¸²ç›¸ä¼¼åº¦æœ€é«˜çš„å‰ top_k ç­† (recipe_id, similarity_score)
    """
    q_emb = model.encode([query])[0]
    q_norm = np.linalg.norm(q_emb)
    sims = embeddings.dot(q_emb) / (emb_norms * q_norm + 1e-10)
    idxs = np.argsort(-sims)[:top_k]
    return [(int(tags[i]["id"]), float(sims[i])) for i in idxs]


def search_and_retrieve(query: str, top_k: int = 3):
    """
    æŸ¥è©¢æ™‚åƒ…å›å‚³ã€Œæ‰€æœ‰é—œéµå­—éƒ½å‡ºç¾ã€çš„é£Ÿè­œï¼ˆingredients/tag å¿…é ˆå…¨è¦†è“‹ï¼‰ï¼Œ
    ä¸¦æ ¹æ“šèªæ„åˆ†æ•¸æ’åºï¼Œæ•¸é‡ä¸è¶³æ™‚ä¸è£œéƒ¨åˆ†å‘½ä¸­ï¼Œåªé¡¯ç¤ºå®Œå…¨å‘½ä¸­çš„çµæœã€‚
    """
    # 1. å–å¾—æ‰€æœ‰ tagsï¼ˆå·²åœ¨å…¨åŸŸè®Šæ•¸ tags è¼‰å…¥ï¼‰
    from collections import defaultdict

    # 2. å°‡ tags è½‰æ›ç‚º id -> set(tag) çµæ§‹
    id2tags = defaultdict(set)
    for item in tags:
        id2tags[item["id"]].add(item["tag"])

    # 3. å°‡ query æ‹†æˆå¤šå€‹é—œéµå­—
    # æ”¯æ´ä¸­/è‹±é€—è™Ÿ
    keywords = [kw.strip() for kw in query.replace("ï¼Œ", ",").split(",") if kw.strip()]
    if not keywords:
        return []

    # 4. æ‰¾å‡ºåŒæ™‚æ“æœ‰æ‰€æœ‰é—œéµå­—çš„é£Ÿè­œ id
    full_hit_ids = [
        rid
        for rid, tagset in id2tags.items()
        if all(any(kw in tag for tag in tagset) for kw in keywords)
    ]

    if not full_hit_ids:
        return []

    # 5. ç”¨ embedding è¨ˆç®—èªæ„åˆ†æ•¸ï¼Œåªæ’åºå®Œå…¨å‘½ä¸­çš„ id
    q_emb = model.encode([query])[0]
    q_norm = np.linalg.norm(q_emb)
    sims = embeddings.dot(q_emb) / (emb_norms * q_norm + 1e-10)
    # è£½ä½œ id: max_score å­—å…¸
    id2score = {}
    for i, t in enumerate(tags):
        rid = int(t["id"])
        if rid in full_hit_ids:
            # å–é€™å€‹ id çš„æœ€å¤§èªæ„åˆ†æ•¸ï¼ˆå› ç‚ºä¸€å€‹ id å¯èƒ½å°æ‡‰å¤šå€‹ tag å‘é‡ï¼‰
            id2score[rid] = max(id2score.get(rid, float("-inf")), float(sims[i]))

    # 6. æŒ‰èªæ„åˆ†æ•¸æ’åºï¼Œå– top_k
    sorted_ids = sorted(full_hit_ids, key=lambda rid: -id2score[rid])[:top_k]

    # 7. å›å‚³å®Œæ•´é£Ÿè­œå…§å®¹èˆ‡åˆ†æ•¸
    results = []
    for rid in sorted_ids:
        recipe = get_recipe_by_id(rid, (df_cleaned, df_preview, df_detailed, df_steps))
        if recipe:
            results.append({"id": rid, "score": id2score[rid], "recipe": recipe})
    return results


def search_by_partial_ingredients(query, top_k=3):
    ingredients = [
        kw.strip() for kw in query.replace("ï¼Œ", ",").split(",") if kw.strip()
    ]
    if not ingredients:
        return []
    id2count = {}
    for rid, tagset in id2tags.items():
        count = sum(any(kw in tag for tag in tagset) for kw in ingredients)
        if count > 0:
            id2count[rid] = count  # è‡³å°‘å‘½ä¸­1å€‹æ‰ç´å…¥
    if not id2count:
        return []
    q_emb = model.encode([query])[0]
    q_norm = np.linalg.norm(q_emb)
    sims = embeddings.dot(q_emb) / (emb_norms * q_norm + 1e-10)
    id2score = {}
    for i, t in enumerate(tags):
        rid = int(t["id"])
        if rid in id2count:
            id2score[rid] = max(id2score.get(rid, float("-inf")), float(sims[i]))
    sorted_ids = sorted(
        id2count.keys(), key=lambda rid: (-id2count[rid], -id2score[rid])
    )[:top_k]
    results = []
    for rid in sorted_ids:
        recipe = get_recipe_by_id(rid, (df_cleaned, df_preview, df_detailed, df_steps))
        if recipe:
            results.append(
                {
                    "id": rid,
                    "score": id2score[rid],
                    "matched_count": id2count[rid],
                    "recipe": recipe,
                }
            )
    return results


def call_ollama_llm(
    user_query: str, recipes: list, model: str = "qwen3:4b-q4_K_M"
) -> str:
    """
    çµ¦LLMæ˜ç¢ºæŒ‡ä»¤ï¼šã€Œè«‹æ¨è–¦æœ€é©åˆuser_queryçš„é£Ÿè­œï¼Œåˆ—å‡ºåç¨±ã€ç°¡ä»‹ã€ç†ç”±ã€‚ã€
    user_queryï¼šç”¨æˆ¶è¼¸å…¥çš„é£Ÿæ/éœ€æ±‚
    recipesï¼šæª¢ç´¢åˆ°çš„å®Œæ•´é£Ÿè­œlistï¼ˆé€šå¸¸top3ï¼‰
    """
    if not recipes:
        return "æ‰¾ä¸åˆ°ç¬¦åˆçš„é£Ÿè­œã€‚"

    # çµ„è£ contextï¼Œæ¯é“èœåç¨±ï¼‹ä¸»è¦é£Ÿæï¼ˆåªéœ€é‡é»è³‡è¨Šå³å¯ï¼‰
    context_blocks = []
    for r in recipes:
        rec = r["recipe"]
        # åªå–é‡é»å…§å®¹ï¼šåç¨±ã€é£Ÿæ
        ingredients_str = "ã€".join(
            i["ingredient_name"] for i in rec.get("ingredients", [])
        )
        context_blocks.append(
            f"ã€{rec.get('é£Ÿè­œåç¨±','')}ã€‘(ID: {r['id']})\n"
            f"ä¸»è¦é£Ÿæï¼š{ingredients_str}\n"
            f"ç°¡è¦èªªæ˜ï¼šå¯åƒè€ƒè©³ç´°æ­¥é©Ÿè£½ä½œã€‚"
        )
    context_text = "\n\n---\n\n".join(context_blocks)

    # æ¨è–¦å‹ promptï¼Œæ˜ç¢ºæŒ‡ä»¤ LLMã€Œè«‹åˆ—å‡ºæ¨è–¦åå–®ã€ç°¡ä»‹èˆ‡æ¨è–¦ç†ç”±ã€
    prompt = (
        f"ä»¥ä¸‹æ˜¯æ–™ç†é£Ÿè­œçš„è³‡è¨Šï¼š\n{context_text}\n\n"
        f"è«‹æ‰®æ¼”ä¸€ä½æ–™ç†å°ˆå®¶ï¼Œæ ¹æ“šé€™äº›é£Ÿè­œè³‡è¨Šï¼Œ"
        f"ç”¨20å­—æè¿°å…§å®¹ã€‚\n"
        f"è«‹åœ¨æ¯é“æ–™ç†æ¨™é¡Œå¾Œæ¨™è¨»å…¶ IDï¼ˆå¦‚ï¼šå°å¼ç¾…å‹’ç‡’é› (ID: 474705)ï¼‰ï¼Œä»¥ä¾¿ç”¨æˆ¶å¾ŒçºŒæŸ¥è©¢ã€‚\n"
        f"å°‡æ‰€æœ‰é£Ÿè­œæ¢åˆ—å¼åˆ†åˆ¥æè¿°å…§å®¹ã€‚\n"
        f"å›è¦†è«‹ç›´æ¥é€²å…¥ä¸»é¡Œï¼Œä¸éœ€è¨è«–åˆ†æéç¨‹ã€‚\n"
        f"åªèƒ½å¾ä¸‹åˆ—æä¾›çš„é£Ÿè­œä¸­æè¿°å…§å®¹ã€‚\n"
        f"è‹¥ç™¼ç¾å…§å®¹é‡è¤‡ï¼Œè«‹åˆä½µç‚ºä¸€æ¢ä¸¦åªåˆ—ä¸€æ¬¡ã€‚\n"
        f"è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"
    )

    try:
        result = subprocess.run(
            ["ollama", "run", model, prompt],
            capture_output=True,
            text=True,
            encoding="utf-8",
            check=True,
        )
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        return f"Ollama ç™¼ç”ŸéŒ¯èª¤ï¼š{e.stderr.strip()}"


def google_search_recipes(keyword: str, k: int = 5) -> List[Dict]:
    """
    å¾Œå‚™ Google æœå°‹ï¼šåœ¨ä½¿ç”¨è€…è¼¸å…¥çš„æ–‡å­—å¾Œé¢è‡ªå‹•åŠ ä¸Šã€Œé£Ÿè­œã€äºŒå­—ï¼Œ
    ä¸¦å–å›å‰ k ç­†çµæœã€‚
    å›å‚³æ ¼å¼ï¼š[{ 'title': ..., 'link': ..., 'snippet': ... }, ...]
    """
    query = f"{keyword} é£Ÿè­œ"
    results = []
    for item in search(query, advanced=True, num_results=k, lang="zh-tw"):
        results.append(
            {"title": item.title, "link": item.url, "snippet": item.description}
        )
    return results


def summarize_search_results(
    user_query: str, results: list, model: str = "qwen3:4b-q4_K_M"
) -> str:
    """æŠŠå¤šç­†æœå°‹çµæœäº¤çµ¦ LLMï¼Œè«‹å®ƒç”¨ç¹é«”ä¸­æ–‡æ­¸ç´å›ç­”"""
    blocks = []
    for r in results:
        blocks.append(f"ã€{r['title']}ã€‘\n{r['snippet']}\nLink: {r['link']}")
    context = "\n\n---\n\n".join(blocks)

    prompt = textwrap.dedent(
        f"""\
        ä½ å°‡ç²å¾—ä¾†è‡ª Google æœå°‹ã€Œ{user_query} é£Ÿè­œã€çš„çµæœæ‘˜è¦ï¼ˆå¦‚ä¸‹ %%% æ‰€ç¤ºï¼‰ï¼Œè«‹ä¾æ“š**åƒ…æä¾›çš„è³‡è¨Š**ç”¢å‡ºæ¢åˆ—å¼æ¸…å–®ã€‚

        âœ… æ¯ç­†è¼¸å‡ºè«‹åš´æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼ˆç”¨å…¨å½¢é€—è™Ÿåˆ†éš”ï¼‰ï¼š
        ç¶²é æ¨™é¡Œï¼Œå…¨å½¢é€—è™Ÿï¼Œ20 å­—å·¦å³çš„ç°¡ä»‹ï¼Œå…¨å½¢é€—è™Ÿï¼ŒåŸå§‹ç¶²å€

        âš ï¸ è«‹æ³¨æ„ï¼š
        1. **åªèƒ½åŸºæ–¼æä¾›çš„è³‡è¨Šå…§å®¹å›ç­”ï¼Œä¸å¾—æ¨è«–æˆ–è‡ªè¡Œè£œå……**
        2. æ¯å‰‡ç°¡ä»‹**é•·åº¦ç´„ç‚º 20 å­—ï¼ˆ18ï½22 å­—å…§ï¼‰**
        3. çµæœä»¥æ¢åˆ—æ¸…å–®å½¢å¼å‘ˆç¾ï¼Œæ¯ç­†çµæœç¨ç«‹ä¸€è¡Œ
        4. è«‹å…¨ç¨‹ä½¿ç”¨**ç¹é«”ä¸­æ–‡**
        5. ç¶²å€è«‹ä¿æŒåŸæ¨£ï¼Œä¸å¯ä¿®æ”¹æˆ–çœç•¥

        %%%
        {context}
        %%%
        """
    )

    res = subprocess.run(
        ["ollama", "run", model, prompt],
        capture_output=True,
        text=True,
        encoding="utf-8",
    )
    return res.stdout.strip()


def pretty_print(item: dict):
    """ä»¥å¯è®€æ–‡å­—æ ¼å¼åˆ—å°å–®ä¸€é“é£Ÿè­œçµæœï¼Œä¸å« preview_tags"""
    rec = item["recipe"]
    print(f"=== æŸ¥è©¢çµæœï¼šRecipe ID {item['id']} (ç›¸ä¼¼åº¦ {item['score']:.4f}) ===\n")
    print(
        f"é£Ÿè­œåç¨±ï¼š{rec.get('é£Ÿè­œåç¨±','')}\n"
        f"åˆ†é¡ã€€ã€€ã€€ï¼š{rec.get('category','')}\n"
        f"ç¶²å€ã€€ã€€ã€€ï¼š{rec.get('ç¶²å€','')}\n"
        f"åœ–ç‰‡è·¯å¾‘ã€€ï¼š{rec.get('åœ–ç‰‡ç›¸å°è·¯å¾‘','')}\n"
    )
    # é£Ÿæ
    print("â”€â”€ é£Ÿæ Ingredients â”€â”€")
    for idx, ing in enumerate(rec.get("ingredients", []), 1):
        print(
            f"{idx}. {ing.get('ingredient_name','')} {ing.get('quantity','')}{ing.get('unit','')}"
        )
    print()
    # æ­¥é©Ÿ
    print("â”€â”€ æ­¥é©Ÿ Steps â”€â”€")
    for step in rec.get("steps", []):
        print(f"{step.get('step_no','')}. {step.get('description','')}")
    print()


if __name__ == "__main__":
    print("RAG æ™ºèƒ½æ¨è–¦æŸ¥è©¢ï¼ˆè¼¸å…¥ä»»ä½•ä¸­æ–‡æè¿°ï¼›exit é›¢é–‹ï¼‰")

    while True:
        raw_input_text = input("\nè«‹æè¿°ä½ æœ‰çš„é£Ÿææˆ–éœ€æ±‚: ").strip()
        if raw_input_text.lower() in ("exit", "quit"):
            break

        # 1) å…ˆæŠ½å–é£Ÿæé—œéµå­—
        keywords = pull_ingredients(raw_input_text)
        if not keywords:
            print("âš ï¸ æœªåµæ¸¬åˆ°ä»»ä½•å¯ç”¨é£Ÿæï¼Œæ”¹ç‚ºç¶²è·¯æœå°‹æ¨¡å¼â€¦")
            # ç›´æ¥åš Google å¾Œå‚™
            web_hits = google_search_recipes(raw_input_text, k=5)
            if not web_hits:
                print("ğŸš« Google ç„¡çµæœï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")
                continue
            summary = summarize_search_results(raw_input_text, web_hits)
            print("ğŸŒ ä¾†è‡ª Google çš„æ¨è–¦ï¼š\n" + summary + "\n")
            # open_choice = input("è¦åœ¨ç€è¦½å™¨é–‹å•Ÿç¬¬ä¸€ç­†çµæœå—ï¼Ÿ(y/n): ").strip().lower()
            # if open_choice == "y":
            #     import webbrowser

            #     webbrowser.open(web_hits[0]["link"])
            continue

        # 2) æœ‰æŠ½åˆ°é—œéµå­—ï¼Œå°±ç”¨æœ¬åœ° OR æª¢ç´¢
        query = ", ".join(keywords)
        res = search_by_partial_ingredients(query, top_k=3)

        # 3) å¦‚æœæœ¬åœ°æŸ¥ç„¡çµæœï¼Œå†è·‘ Google å¾Œå‚™
        if not res:
            print("âš ï¸ æœ¬åœ°è³‡æ–™åº«æŸ¥ç„¡çµæœï¼Œå˜—è©¦ç¶²è·¯æœå°‹â€¦")
            web_hits = google_search_recipes(query, k=5)
            if not web_hits:
                print("ğŸš« Google ç„¡çµæœï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")
                continue
            summary = summarize_search_results(query, web_hits)
            print("ğŸŒ ä¾†è‡ª Google çš„æ¨è–¦ï¼š\n" + summary + "\n")
            # open_choice = input("è¦åœ¨ç€è¦½å™¨é–‹å•Ÿç¬¬ä¸€ç­†çµæœå—ï¼Ÿ(y/n): ").strip().lower()
            # if open_choice == "y":
            #     import webbrowser

            #     webbrowser.open(web_hits[0]["link"])
            continue

        print("\næ­£åœ¨è‡ªå‹•æ¨è–¦æœ€é©åˆçš„é£Ÿè­œ...\n")
        answer = call_ollama_llm(query, res)
        print("ğŸ§  æ™ºèƒ½æ¨è–¦ï¼š\n" + answer + "\n")

        print(
            "ğŸ” è‹¥æƒ³æŸ¥çœ‹å…¶ä¸­ä¸€é“é£Ÿè­œçš„ã€è©³ç´°é£Ÿæèˆ‡æ­¥é©Ÿã€‘ï¼Œ"
            "è«‹è¼¸å…¥è©²é£Ÿè­œã€åç¨±é—œéµå­—ã€æˆ–è©²é£Ÿè­œçš„ ID"
        )
        print("âœï¸ è‹¥æƒ³é‡æ–°æŸ¥è©¢å…¶ä»–é£Ÿæï¼Œè«‹è¼¸å…¥ newï¼›é›¢é–‹è«‹è¼¸å…¥ exitã€‚")

        name_map = {r["recipe"]["é£Ÿè­œåç¨±"]: r["id"] for r in res}
        id_set = set(r["id"] for r in res)
        selected_id = None

        while True:
            follow_up = input(
                "è«‹è¼¸å…¥æƒ³æŸ¥çœ‹è©³æƒ…çš„é£Ÿè­œç·¨è™Ÿ/åç¨±ï¼Œæˆ–è¼¸å…¥ new æŸ¥è©¢æ–°é£Ÿæ: "
            ).strip()
            if follow_up.lower() in ("exit", "quit"):
                exit()
            if follow_up.lower() in ("new", ""):
                break

            if follow_up.isdigit() and int(follow_up) in id_set:
                selected_id = int(follow_up)
            else:
                for name, rid in name_map.items():
                    if follow_up in name:
                        selected_id = rid
                        break

            if selected_id:
                recipe = get_recipe_by_id(
                    selected_id,
                    (df_cleaned, df_preview, df_detailed, df_steps),
                )
                if recipe:
                    pretty_print({"id": selected_id, "score": 1.0, "recipe": recipe})
                    print(
                        "\nğŸ“Œ æ‚¨å¯ä»¥è¼¸å…¥å…¶ä»– ID æˆ–åç¨±ç¹¼çºŒæŸ¥çœ‹ï¼Œæˆ–è¼¸å…¥ new æŸ¥è©¢æ–°å…§å®¹ã€‚"
                    )
                else:
                    print("æ‰¾ä¸åˆ°è©²é£Ÿè­œçš„è©³ç´°è³‡è¨Šã€‚")
            else:
                print("ç„¡æ³•è¾¨è­˜è¼¸å…¥å…§å®¹ï¼Œè«‹å†è¼¸å…¥ä¸€æ¬¡ã€‚")
